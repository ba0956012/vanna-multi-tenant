#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Few-shot è‡ªå‹•ç”Ÿæˆï¼ˆPostgreSQL ç‰ˆæœ¬ï¼‰- MASK + LIMIT FIELDS
=========================================================

ç‰¹æ€§ç¸½è¦½ï¼š
1. ä½¿ç”¨ PostgreSQL information_schema å–å¾— FK é—œä¿‚
2. åªä½¿ç”¨ FK å»º JOIN graphï¼ˆç„¡å‘ï¼‰â†’ BFS â†’ JOIN Route
3. SELECT ä¸€å¾‹ï¼št0.*, t1.*, t2.* ...
4. WHERE è¦å‰‡ï¼š
   - æ°¸é åªä½¿ç”¨ root tableï¼ˆt0ï¼‰æ¬„ä½
   - æœ€å¤šè‡ªå‹•æŒ‘é¸ 2â€“3 å€‹æ¬„ä½
   - æ’é™¤ id / created_at / updated_at / timestamp é¡å‹
   - TEXT/VARCHAR â†’ LIKE '%[column]%'
   - é TEXT â†’ = [column]
5. WHERE ä¸€å¾‹ä½¿ç”¨ MASKï¼ˆplaceholderï¼‰ï¼Œä¸ä½¿ç”¨å¯¦éš›å€¼
6. SQL æœƒå…ˆç”¨ PostgreSQL execute é©—è­‰ï¼ŒéŒ¯èª¤å³è·³é
7. æ¯å¼µè¡¨ç”¢ç”Ÿ 1 ç­† few-shot
8. è¼¸å‡ºå¯ç›´æ¥åŒ¯å…¥ ChromaDB è¨˜æ†¶
"""

import json
import os
import argparse
import random
import psycopg2
from pathlib import Path


# =====================================================
#  å–å¾— PostgreSQL Schema
# =====================================================

def analyze_database(conn):
    cur = conn.cursor()

    # å–å¾—æ‰€æœ‰ public schema çš„è¡¨
    cur.execute("""
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
        ORDER BY table_name;
    """)
    tables = [r[0] for r in cur.fetchall()]

    schema = {}
    for t in tables:
        # å–å¾—æ¬„ä½è³‡è¨Š
        cur.execute("""
            SELECT column_name, data_type, is_nullable, column_default
            FROM information_schema.columns 
            WHERE table_schema = 'public' AND table_name = %s
            ORDER BY ordinal_position;
        """, (t,))
        cols = cur.fetchall()

        # å–å¾— FK è³‡è¨Š
        cur.execute("""
            SELECT
                kcu.column_name as from_column,
                ccu.table_name as to_table,
                ccu.column_name as to_column
            FROM information_schema.table_constraints tc
            JOIN information_schema.key_column_usage kcu
                ON tc.constraint_name = kcu.constraint_name
                AND tc.table_schema = kcu.table_schema
            JOIN information_schema.constraint_column_usage ccu
                ON ccu.constraint_name = tc.constraint_name
                AND ccu.table_schema = tc.table_schema
            WHERE tc.constraint_type = 'FOREIGN KEY'
                AND tc.table_schema = 'public'
                AND tc.table_name = %s;
        """, (t,))
        fks = cur.fetchall()

        schema[t] = {"columns": cols, "fks": fks}

    return tables, schema


# =====================================================
#  å»º FK Graphï¼ˆç„¡å‘ï¼‰
# =====================================================

def build_fk_graph(tables, schema):
    graph = {t: [] for t in tables}

    for t in tables:
        for fk in schema[t]["fks"]:
            from_col, ref_table, to_col = fk

            if ref_table in graph:
                graph[t].append((ref_table, from_col, to_col))
                graph[ref_table].append((t, to_col, from_col))

    return graph


# =====================================================
#  BFS JOIN é †åº
# =====================================================

def bfs_join_tables(root, graph):
    visited = set()
    queue = [root]
    order = []

    while queue:
        t = queue.pop(0)
        if t in visited:
            continue
        visited.add(t)
        order.append(t)

        for to_table, _, _ in graph[t]:
            if to_table not in visited:
                queue.append(to_table)

    return order


# =====================================================
#  å–å¾— sample rowï¼ˆåƒ…ç”¨ä¾†åˆ¤æ–·å“ªäº›æ¬„ä½é NULLï¼‰
# =====================================================

def get_sample_row(conn, table):
    cur = conn.cursor()
    try:
        cur.execute(f'SELECT * FROM "{table}" LIMIT 1;')
        row = cur.fetchone()
        if row is None:
            return None
        cols = [c[0] for c in cur.description]
        return dict(zip(cols, row))
    except Exception as e:
        print(f"  âš ï¸ å–å¾— sample row å¤±æ•—: {e}")
        return None


# =====================================================
#  WHERE æ¬„ä½é¸æ“‡èˆ‡ MASK è¦å‰‡
# =====================================================

EXCLUDE_COLUMN_NAMES = {"id", "created_at", "updated_at"}
EXCLUDE_TYPES = {"date", "timestamp with time zone", "timestamp without time zone"}


def select_where_columns(schema_cols, sample_row, max_fields=3):
    candidates = []
    for col in schema_cols:
        name = col[0]
        ctype = (col[1] or "").lower()

        if name.lower() in EXCLUDE_COLUMN_NAMES:
            continue
        if ctype in EXCLUDE_TYPES:
            continue
        if sample_row.get(name) is None:
            continue

        candidates.append(col)

    random.shuffle(candidates)
    return candidates[:max_fields]


def build_where_clause(schema_cols, sample_row, max_fields=3):
    selected = select_where_columns(schema_cols, sample_row, max_fields)
    if not selected:
        return ""

    parts = []
    for col in selected:
        name = col[0]
        ctype = (col[1] or "").lower()
        val = sample_row.get(name)

        if "char" in ctype or "text" in ctype:
            # ä½¿ç”¨ placeholderï¼Œä¸åŠ å¼•è™Ÿé¿å… SQL èªæ³•éŒ¯èª¤
            parts.append(f't0."{name}" LIKE \'%[{name}]%\'')
        else:
            parts.append(f't0."{name}" = [{name}]')

    return "WHERE " + " AND ".join(parts)


# =====================================================
#  JOIN SQL ç”Ÿæˆ
# =====================================================

def generate_join_sql(root, join_order, schema, graph, sample_row, max_where_fields=3):
    aliases = {t: f"t{i}" for i, t in enumerate(join_order)}

    sql = []
    sql.append("SELECT " + ", ".join(f'{aliases[t]}.*' for t in join_order))
    sql.append(f'FROM "{root}" {aliases[root]}')

    for t in join_order:
        if t == root:
            continue

        parent = None
        parent_fk = None
        for pt in join_order:
            if pt == t:
                break
            for to_table, from_col, to_col in graph[pt]:
                if to_table == t:
                    parent = pt
                    parent_fk = (from_col, to_col)
                    break
            if parent:
                break

        if not parent:
            continue

        p_alias = aliases[parent]
        t_alias = aliases[t]
        from_col, to_col = parent_fk

        sql.append(
            f'LEFT JOIN "{t}" {t_alias} ON {p_alias}."{from_col}" = {t_alias}."{to_col}"'
        )

    where_sql = build_where_clause(schema[root]["columns"], sample_row, max_where_fields)
    if where_sql:
        sql.append(where_sql)

    sql.append("LIMIT 200;")
    return "\n".join(sql)


# =====================================================
#  SQL é©—è­‰
# =====================================================

def validate_sql(conn, sql):
    try:
        cur = conn.cursor()
        # ç”¨ EXPLAIN é©—è­‰ SQL èªæ³•ï¼Œä¸å¯¦éš›åŸ·è¡Œ
        cur.execute(f"EXPLAIN {sql}")
        cur.close()
        return True
    except Exception as e:
        print(f"  âŒ SQL é©—è­‰å¤±æ•—: {e}")
        # Rollback ä»¥é¿å… transaction ä¸­æ­¢ï¼Œè®“å¾ŒçºŒæŸ¥è©¢å¯ä»¥ç¹¼çºŒ
        conn.rollback()
        return False


# =====================================================
#  Schema æè¿°
# =====================================================

def generate_schema_description(schema):
    lines = ["/* Database Schema */\n"]
    for t, info in schema.items():
        lines.append(f"-- {t}")
        lines.append(f"CREATE TABLE {t} (")
        col_defs = []
        for c in info["columns"]:
            col_defs.append(f"  {c[0]} {c[1]}")
        lines.append(",\n".join(col_defs))
        lines.append(");\n")

        if info["fks"]:
            lines.append("/* FOREIGN KEYS:")
            for fk in info["fks"]:
                lines.append(f" * {t}.{fk[0]} -> {fk[1]}.{fk[2]}")
            lines.append(" */\n")

    return "\n".join(lines)


# =====================================================
#  Few-shot ç”Ÿæˆ
# =====================================================

def generate_fewshot_for_table(table, conn, schema, graph, db_name):
    print(f"ğŸ§© Table: {table}")

    sample = get_sample_row(conn, table)
    if not sample:
        print("  âš ï¸ ç„¡è³‡æ–™ï¼Œè·³é")
        return None

    join_order = bfs_join_tables(table, graph)
    sql = generate_join_sql(table, join_order, schema, graph, sample)

    # ç§»é™¤ MASK ä¾†é©—è­‰ SQL
    # æ ¹æ“šæ¬„ä½é¡å‹ä½¿ç”¨ä¸åŒçš„æ¸¬è©¦å€¼
    test_sql = sql
    for col in schema[table]["columns"]:
        name = col[0]
        ctype = (col[1] or "").lower()
        
        # æ ¹æ“šè³‡æ–™é¡å‹é¸æ“‡æ¸¬è©¦å€¼
        if "char" in ctype or "text" in ctype:
            # LIKE '%[name]%' â†’ LIKE '%test%'
            test_sql = test_sql.replace(f"'%[{name}]%'", "'%test%'")
        elif "int" in ctype or "serial" in ctype:
            # = [name] â†’ = 1ï¼ˆæ•´æ•¸é¡å‹ï¼‰
            test_sql = test_sql.replace(f"[{name}]", "1")
        elif "numeric" in ctype or "decimal" in ctype or "float" in ctype or "double" in ctype or "real" in ctype:
            # = [name] â†’ = 1.0ï¼ˆæ•¸å€¼é¡å‹ï¼‰
            test_sql = test_sql.replace(f"[{name}]", "1.0")
        elif "bool" in ctype:
            # = [name] â†’ = trueï¼ˆå¸ƒæ—é¡å‹ï¼‰
            test_sql = test_sql.replace(f"[{name}]", "true")
        elif "date" in ctype or "time" in ctype:
            # = [name] â†’ = '2024-01-01'ï¼ˆæ—¥æœŸæ™‚é–“é¡å‹ï¼‰
            test_sql = test_sql.replace(f"[{name}]", "'2024-01-01'")
        else:
            # å…¶ä»–é¡å‹é è¨­ç”¨å­—ä¸²
            test_sql = test_sql.replace(f"[{name}]", "'test'")

    if not validate_sql(conn, test_sql):
        return None

    # ç”Ÿæˆå•é¡Œæè¿°
    question = generate_question_from_sql(table, join_order, schema)

    return {
        "question": question,
        "tool_name": "run_sql",
        "args_json": json.dumps({"sql": sql}),
        "db_id": db_name,
    }


def generate_question_from_sql(root_table, join_order, schema):
    """æ ¹æ“š SQL çµæ§‹ç”Ÿæˆè‡ªç„¶èªè¨€å•é¡Œ"""
    if len(join_order) == 1:
        return f"æŸ¥è©¢ {root_table} çš„è³‡æ–™"
    else:
        related = ", ".join(join_order[1:])
        return f"æŸ¥è©¢ {root_table} åŠå…¶é—œè¯çš„ {related} è³‡æ–™"


# =====================================================
#  åŒ¯å‡ºåˆ° ChromaDB è¨˜æ†¶
# =====================================================

def export_to_chromadb_memory(fewshots, agent_id, agent_data_dir):
    """å°‡ few-shot åŒ¯å…¥åˆ°æŒ‡å®š agent çš„ ChromaDB"""
    from vanna.integrations.chromadb.agent_memory import ChromaAgentMemory
    from vanna.core.user import User, RequestContext
    import asyncio

    persist_dir = f"{agent_data_dir}/chroma_db_{agent_id}"
    memory = ChromaAgentMemory(
        persist_directory=persist_dir,
        collection_name=f"vanna_{agent_id}"
    )

    context = RequestContext(user=User(id="admin"))

    async def save_all():
        for fs in fewshots:
            args = json.loads(fs["args_json"])
            await memory.save_tool_usage(
                question=fs["question"],
                tool_name=fs["tool_name"],
                args=args,
                context=context,
                success=True,
                metadata={"db_id": fs["db_id"], "auto_generated": True},
            )
            print(f"  âœ“ {fs['question'][:50]}...")

    asyncio.run(save_all())


# =====================================================
#  main
# =====================================================

def main():
    parser = argparse.ArgumentParser(description="PostgreSQL Few-shot è‡ªå‹•ç”Ÿæˆ")
    parser.add_argument("--host", type=str, required=True, help="PostgreSQL host")
    parser.add_argument("--port", type=str, default="5432", help="PostgreSQL port")
    parser.add_argument("--user", type=str, required=True, help="PostgreSQL user")
    parser.add_argument("--password", type=str, required=True, help="PostgreSQL password")
    parser.add_argument("--database", type=str, required=True, help="PostgreSQL database")
    parser.add_argument("--agent_id", type=str, help="Agent ID (ç”¨æ–¼åŒ¯å…¥ ChromaDB)")
    parser.add_argument("--agent_data_dir", type=str, default="./agent_data", help="Agent data ç›®éŒ„")
    parser.add_argument("--output", type=str, help="è¼¸å‡º JSON æª”æ¡ˆè·¯å¾‘")
    args = parser.parse_args()

    # é€£æ¥è³‡æ–™åº«
    conn_string = f"postgresql://{args.user}:{args.password}@{args.host}:{args.port}/{args.database}"
    print(f"ğŸ”— é€£æ¥è³‡æ–™åº«: {args.host}:{args.port}/{args.database}")

    try:
        conn = psycopg2.connect(conn_string)
    except Exception as e:
        print(f"âŒ é€£æ¥å¤±æ•—: {e}")
        return

    tables, schema = analyze_database(conn)
    print(f"ğŸ“Š æ‰¾åˆ° {len(tables)} å¼µè¡¨: {', '.join(tables)}")

    graph = build_fk_graph(tables, schema)

    fewshots = []
    for t in tables:
        fs = generate_fewshot_for_table(t, conn, schema, graph, args.database)
        if fs:
            fewshots.append(fs)

    conn.close()

    print(f"\nâœ… å®Œæˆï¼Œå…±ç”¢ç”Ÿ {len(fewshots)} ç­† few-shot")

    # è¼¸å‡ºåˆ° JSON æª”æ¡ˆ
    if args.output:
        out_path = Path(args.output)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(fewshots, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ è¼¸å‡ºä½ç½®ï¼š{out_path}")

    # åŒ¯å…¥åˆ° ChromaDB
    if args.agent_id:
        print(f"\nğŸ“¥ åŒ¯å…¥åˆ° Agent: {args.agent_id}")
        export_to_chromadb_memory(fewshots, args.agent_id, args.agent_data_dir)
        print("âœ… åŒ¯å…¥å®Œæˆ")


if __name__ == "__main__":
    main()
